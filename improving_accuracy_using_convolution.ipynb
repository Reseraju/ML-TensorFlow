{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNdWcAqOXZ9XvW4B2Cr5QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Reseraju/ML-TensorFlow/blob/main/improving_accuracy_using_convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef2c-yE6vYCB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to make the accuracy on training and validation better is convolution. The ultimate concept is that they narrow down the content of the image to focus on specific parts and this will likely improve the model accuracy.\n",
        "\n",
        "In convolution you take an array (usually 3x3 or 5x5) and scan it over the entire image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection.\n",
        "\n",
        "This is perfect for computer vision because it often highlights features that distinguish one item from another. Moreover, the amount of information needed is then much less because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focused and possibly more accurate."
      ],
      "metadata": {
        "id": "oCYqBPg1vuVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "  # Add convolutions and max pooling\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "  # Add the same layers as before\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Use same settings\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(f'\\nMODEL TRAINING:')\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(f'\\nMODEL EVALUATION:')\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "id": "bHFmB8Whvi5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's likely gone up to about 92% on the training data and 90% on the validation data.\n",
        "\n",
        "Here instead of the input layer at the top, you added a Conv2D layer.The parameters are:\n",
        "\n",
        "1.   The number of convolutions you want to generate. The value here is purely\n",
        "arbitrary but it's good to use powers of 2 starting from 32.\n",
        "2.   The size of the Convolution. In this case, a 3x3 grid.\n",
        "3.  The activation function to use. In this case, you used a ReLU, which you might recall is the equivalent of returning x when x>0, else return 0.\n",
        "4. In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the convolution with a MaxPool2D layer which is designed to compress the image, while maintaining the content of the features that were highlighted by the convolution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image.\n",
        "The idea is that it creates a 2x2 array of pixels, and picks the biggest one. Thus, it turns 4 pixels into 1. It repeats this across the image, and in doing so, it halves both the number of horizontal and vertical pixels, effectively reducing the image to 25% of the original image.\n",
        "\n",
        "call model.summary() to see the size and shape of the network, and you'll notice that after every max pooling layer, the image size is reduced in this way.\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SIrGvCU6xkQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you added another convolution and flattened the output.\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pSJyy1Wd2iTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, you'll just have the same DNN structure as the non convolutional version. The same dense layer with 128 neurons, and output layer with 10 neurons as in the pre-convolution example:\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JpEUHp8r2vsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "Convolutions can be shown graphically. The cell below prints the first 100 labels in the test set, and you can see that the ones at index `0`, index `23` and index `28` are all the same value (i.e. `9`). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the dense layer is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ],
      "metadata": {
        "id": "pUIZ0iFu7Nnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[:100])"
      ],
      "metadata": {
        "id": "PlYRFg2pxMAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models\n",
        "\n",
        "f, axarr = plt.subplots(3,4)\n",
        "\n",
        "FIRST_IMAGE=4\n",
        "SECOND_IMAGE=2\n",
        "THIRD_IMAGE=9\n",
        "CONVOLUTION_NUMBER = 2\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "metadata": {
        "id": "id4xpcW-7VSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0FAt_kT7Y2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}